%!TEX program = xelatex
\documentclass[10pt]{beamer}

%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{4 on 1 with notes}[a4paper,border shrink=5mm]

%own packages
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{textcomp}
\mathtoolsset{showonlyrefs}
\usepackage{grffile}
\usepackage{csquotes}
\usepackage[english]{babel}
\usepackage{caption}
\usepackage{graphicx} %trim for includegraphics
\usepackage{siunitx} % \SI for units
\usepackage{booktabs}
\usepackage{fancybox}


%TikZ stuff
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{chains, positioning, arrows}

%beamer template
%\usepackage{handoutWithNotes}
%\pgfpagesuselayout{4 on 1 with notes}[a4paper,border shrink=5mm]

%bib handling
\usepackage[backend=biber,style=authoryear,bibencoding=utf8, maxcitenames=1]{biblatex}
\addbibresource[datatype=bibtex]{literature.bib}
\DeclareCiteCommand{\xfootcite}[\mkbibfootnote]
  {\usebibmacro{prenote}}
  {\usebibmacro{citeindex}%
   \setunit{\addnbspace}
   \printnames{labelname}%
   \setunit{\labelnamepunct}
  \printfield[citetitle]{title}%
   \newunit
   \printfield{year}
}{\addsemicolon\space}{\usebibmacro{postnote}}

%theme adjustements
\usetheme[block = fill, progressbar = frametitle, ]{metropolis}
\definecolor{TuGreen}{RGB}{132,184,24}
\definecolor{TuGreen40}{RGB}{211,227,175}
\setbeamercolor{title separator}{fg = TuGreen}
\setbeamercolor{progress bar}{fg = TuGreen, bg = TuGreen40}
\setbeamertemplate{frame footer}{Jakob Richter (TU Dortmund)}
\setbeamersize{text margin left=1cm,text margin right=1cm}
\setbeamerfont{title}{size=\large}%
%\usefonttheme[onlymath]{serif}

\newcommand{\rb}[1]{\texttt{rosenbrock}$(#1)$}
\newcommand{\boha}[1]{\texttt{bohachevsky}$(#1)$}
\newcommand{\ack}[1]{\texttt{ackley}$(#1)$}
\newcommand{\ras}[1]{\texttt{rastrigin}$(#1)$}

\setlength{\topsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\partopsep}{1pt}

%set title
\title[mlrHyperopt]{mlrHyperopt: Effortless and collaborative hyperparameter optimization experiments}
\date{\today}
\author[Jakob Richter]{Jakob~Richter}
\institute{Faculty of Statistics, TU Dortmund University}

\titlegraphic{\raisebox{0.5cm}{\includegraphics[height=0.5cm]{assets/tud_logo_pantone}}\hfill\includegraphics[height=1cm]{assets/3zeilig_links_en}}

\begin{document}

<<setup, include=FALSE>>=
library(knitr)
library(pander)
library(xtable)
library(reshape2)
library(ggplot2)
library(stringi)
library(magrittr)
library(data.table)
library(BBmisc)
options(formatR.arrow=TRUE, width=120)
opts_chunk$set(
  fig.path='figure/',
  fig.align='center',
  fig.show='hold',
  fig.lp='fig:',
  fig.width = 8,
  fig.height = 5.5,
  size='small',
  message=FALSE,
  warning=FALSE,
  cache=TRUE,
  cache.path='cache/'
  )

if(getwd() %>% stri_endswith_fixed("mlrHyperopt")) {
  setwd(file.path(getwd(), "meta", "useR2017", "beamer"))
}

myrank = function(x, ties.method) {
  ranks = rank(x, na.last = TRUE, ties.method = ties.method)
  ranks[is.na(x)] = max(ranks)
  ranks
}

trans_theme = theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.background = element_rect(fill=NA),
    plot.background = element_rect(fill=NA),
    legend.background = element_rect(fill=NA)
)
@

\maketitle

\begin{frame}{Contents}
  \begin{enumerate}
    \item Motivation for \texttt{caret}\footnote{\url{https://topepo.github.io/caret}} Users
    \item Motivation for \texttt{mlr}\footnote{\url{https://mlr-org.github.io/mlr}} Users
    \item Website and API
    \item Parameter Tuning
    \item Lessons learned
  \end{enumerate}
\end{frame}

\section{Motivation}

\begin{frame}[fragile]{caret}

\texttt{caret} automatically performs a grid search for all learners.

<<caret1, message=FALSE>>=
library(caret)
system.time({m.c = train(iris[,1:4], iris[,5], method = "rf")})
system.time({m.r = randomForest(iris[,1:4], iris$Species)})
@

How to find out what is going on?

<<caret2>>=
m.c$results
@
\end{frame}

\begin{frame}[fragile]{caret}
Can I find out in advance which parameters will be tuned? \\
\vspace{0.5em}
\begin{overprint}
  \onslide<1>
  \texttt{modelLookup("rf")} gives some information.
  <<caretModelLookup, size='scriptsize'>>=
modelLookup("rf")
@
  \onslide<2>
  \begin{figure}
    \shadowbox{\includegraphics[width=0.8\linewidth]{assets/caret_screen_model_source.png}}
    \caption*{\url{http://github.com/topepo/caret/blob/master/models/files} reveals all details.}
  \end{figure}
\end{overprint}
\end{frame}

\begin{frame}[fragile]{mlr}

\texttt{mlr} provides parameter definitions for all learners.
<<mlrLearner, size='scriptsize'>>=
library(mlr)
lrn = makeLearner("classif.randomForest")
filterParams(getParamSet(lrn), tunable = TRUE)
@
But \texttt{ParamSets} are unconstrained and include possibly unimportant parameters.
\end{frame}

\begin{frame}[fragile]{mlr}

Necessarry to define own \texttt{ParamSets} for tuning

<<paramSet1, message=FALSE>>=
ps = makeParamSet(
  makeIntegerParam("mtry", lower = 1, upper = 4),
  makeIntegerParam("nodesize", lower = 1, upper = 10)
)
tuneParams(lrn, iris.task, cv10, measures = acc,
  par.set = ps, makeTuneControlGrid(resolution = 3L))
@

\end{frame}

\begin{frame}[fragile]{caret}

Deviate from the defaults in \texttt{caret}:

<<caretDeviate, message=FALSE>>=
grid = expand.grid(mtry = 2:4, nodesize = c(1,5,10))
m = caret::train(iris[,1:4], iris[,5],
  method = "rf", tuneGrid = grid)
@

It seems you have to write you own custom method\footnote{\url{https://stackoverflow.com/questions/38625493/tuning-two-parameters-for-random-forest-in-caret-package}}.
\end{frame}

\begin{frame}{mlr vs. caret}
\begin{block}{In caret...}
  \begin{itemize}
    \item[$+$] tuning is the default.
    \item[$+$] tuning with defaults is easy.
    \item[$-$] deviating from defaults is a hassle and needs expert knowedlge.
  \end{itemize}
\end{block}

\begin{block}{In mlr...}
  \begin{itemize}
    \item[$+$] train works like the default of the package.
    \item[$-$] tuning needs expert knowledge.
    \item[$+$] deviating from defaults is easy.
  \end{itemize}
\end{block}

To solve this problem in \texttt{mlr} we want to share the expert knowledge with...
\end{frame}

\section{mlrHyperopt}

\begin{frame}{mlrHyperopt}

\texttt{mlrHyperopt} enables access to a web database of Parameter Configurations for many machine learning methods in R.

\begin{block}{Why an online database?}
  \begin{itemize}
    \item Defaults in packages will always be controversial.
    \item Knowledge changes over time but \texttt{R} packages have to maintain reprudicebility.
    \item Defauls differ for different scenarios. (data set size \emph{etc.})
  \end{itemize}
\end{block}

\end{frame}

\begin{frame}{mlrHyperopt: ParamConfigs}

\texttt{mlrHyperopt} stores tuning parameters in \texttt{ParConfigs}:

\begin{itemize}
  \item \emph{Parameter Set} of tunable parameters
  \item fixed \emph{Parameter Values} to overwrite defaults
  \item Associated learner and note
\end{itemize}

\begin{block}{Features of the Parameter Set\footnote{\url{https://github.com/berndbischl/ParamHelpers}}:}
  \begin{itemize}
    \item Parameter values can be: real-valued, integer, discrete, logical, \ldots
    \item Parameters can have:
    \begin{itemize}
      \item transformations (to account non-uniform distribution of interesting regions)
      \item requriements on other parameters (to represent hirachical structures)
    \end{itemize}
    \item Bounds and defaults can depend on the task size, number of features, \emph{etc.}
  \end{itemize}
\end{block}

\end{frame}

\section{API Examples}

\begin{frame}{Web Interface}

\begin{figure}[h]
  \shadowbox{\includegraphics[width=\linewidth]{assets/mlrHyperopt_screen_parconfigs.png}}
  \caption*{Overview of all \texttt{ParConfigs} uploaded to \url{http://mlrhyperopt.jakob-r.de/parconfigs}}
\end{figure}

\end{frame}

\begin{frame}[fragile]{API: Download and Use ParConfigs}
<<apiDownload, message=FALSE, size='scriptsize'>>=
library(mlrHyperopt)
lrn = makeLearner("classif.ranger")
(pc = downloadParConfigs(learner.class = getLearnerClass(lrn)))
ps = getParConfigParSet(pc[[1]])
ps = evaluateParamExpressions(ps, dict = getTaskDictionary(iris.task))
lrn = setHyperPars(lrn, par.vals = getParConfigParVals(pc[[1]]))
tuneParams(lrn, iris.task, resampling = cv10, par.set = ps,
  measures = acc, control = makeTuneControlRandom(maxit = 10))
@
\end{frame}

\begin{frame}[fragile]{API: Upload ParConfigs}
<<apiUpload, message=FALSE, size='scriptsize', eval=FALSE>>=
ps = makeParamSet(
  makeDiscreteParam("kernel", c("rbfdot", "polydot")),
  makeNumericParam("C", -5, 5, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -10, upper = 10,
    trafo = function(x) 2^x, requires = quote(kernel == "rbfdot")),
  makeNumericParam("degree", lower = 1, upper = 5,
    requires = quote(kernel == "polydot"))
)
pc = makeParConfig(ps, learner.name = "ksvm")
uploadParConfig(pc)
## [1] "23"
@
\end{frame}

\begin{frame}[fragile]{Bonus: Use ParConfigs in caret}
<<apiCaret, message=FALSE, size='scriptsize', warning=FALSE>>=
pc = downloadParConfigs(learner.name = "nnet")
grid = generateRandomDesign(n = 10L, par.set = pc[[1]]$par.set, trafo = TRUE)
tr = caret::train(iris[,1:4], iris[,5], method = "nnet", tuneGrid = grid)
tr$bestTune
@
\end{frame}

\section{\texttt{caret} vs. \texttt{mlrHyperopt}}

<<readBenchmarkResults, include=FALSE>>=
res = readRDS("../../res5.rds")

# merge learner names
lrns = data.frame(
  mlr =   c("ksvm",      "randomForest", "glmnet", "rpart", "nnet", "xgboost", "extraTrees"),
  caret = c("svmRadial", "rf",           "glmnet", "rpart", "nnet", "xgbTree", "extraTrees")
)
lrns2 = as.data.table(lrns)
lrns2 = plyr::rename(lrns2, c(caret = "learner"))
res = merge(res, lrns2, all.x = TRUE, by = "learner")
res[!is.na(mlr), learner := mlr, ]

# punish not terminated
res[is.na(done), measure := 0]

# naming and units
res[, algorithm := plyr::revalue(algorithm, c(mlrDefault = "default"))]
res[algorithm == "default", budget := 1]
res[, Method := algorithm]
res[algorithm == "caret", Method := paste(algorithm,search)]
res[, Combination := sprintf("%s tuned with %s", learner, Method)]
res[, budget := as.factor(budget)]
res$time = as.numeric(res$time, units = "secs")
res[, case := paste0(problem, ": ", learner)]
@

\begin{frame}{User Experience}

\end{frame}

\begin{frame}{Feature Comparison}
\begin{table}[]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
                          & caret               & mlrHyperopt           \\ \midrule
tunable Learners          &                     &                       \\
adjustable Search Space   & pass own data.frame & all                   \\
random search             &                     &                       \\
grid search               & default             &                       \\
model based search        &                     &                       \\
further search algorithms &                     & supported through mlr \\
documented search spaces  &                     &                       \\ \bottomrule
\end{tabular}
\end{table}
\end{frame}

\begin{frame}{Performance}
\texttt{caret} exceeded walltime of \SI{24}{\hour} on Docker Cluster (Intel Xeon E5-2697v2).
Resulting NAs are imputed with a measure of 0.
<<caretErrors, echo=FALSE, fig.width=12, fig.height=8.5, results='asis'>>=
res[is.na(done), .(learner, algorithm)] %>% table() %>% xtable() %>% print(booktabs = TRUE)
@
\end{frame}


\begin{frame}{Performance: All results}
<<results1, echo=FALSE, fig.width=12, fig.height=8.5>>=
g = ggplot(data = res[budget %in% c(1,10)], aes(x = Method, y = measure, color = Method))
g = g + geom_boxplot() + facet_wrap("case", scales = "free", ncol = length(unique(res$learner)))
g = g + theme(axis.text.x=element_blank(), legend.position="bottom") + ylab("accuracy")
g
@
\end{frame}

\begin{frame}{Performance: Overhead}
<<resultsRunTime, echo=FALSE, fig.width=12, fig.height=8.5>>=
g = ggplot(data = res[budget %in% c(10,50)], aes(x = budget, y = time))
g + geom_line(aes(color = Method, group = paste(algorithm,search,fold)), alpha = 0.7) + facet_grid(learner~problem, scales = "free")
@
\end{frame}

\begin{frame}{Performance: Tuning Gain}
Without timed out algorithms: \texttt{extraTrees}, \texttt{nnet} and \texttt{xgboost}. \\
Only datasets that are actually tunable: \texttt{mfeatâˆ’morphological}, \texttt{wilt}, \texttt{JapaneseVowels}
<<resultsRelative, echo=FALSE, fig.height=4.5>>=
res[, measure.rel := measure - median(measure[algorithm == "default"]), by = .(case, fold)]
g = ggplot(data = res[budget %in% c(10,50) & learner %nin% c("extraTrees", "nnet", "xgboost") & problem %in% c("mfeat-morphological", "wilt", "JapaneseVowels")], aes(x = budget, y = measure.rel, color = Method))
g = g + geom_boxplot() + facet_grid(problem~learner, scales = "free")
g = g + geom_hline(yintercept = 0)
g = g + theme(legend.position="bottom") + ylab("accuracy gain")
g
@
\end{frame}

\begin{frame}{Performance: Dominance Relative}
<<makeDataWithDefaultInAllBudgets, include=FALSE>>=
tmp.default = res[algorithm == "default"]
tmp = res[algorithm != "default"]
tmp.default$budget = NULL
tmp.default = merge(tmp.default, data.table(budget = unique(tmp$budget), algorithm = "default"), by = "algorithm", all.y = TRUE, allow.cartesian = TRUE)
res.def = rbind(tmp, tmp.default)
res.def[, budget := factor(budget)]
@
<<resultsDominance, echo=FALSE, results='asis'>>=
combs = expand.grid(MethodA = unique(res.def$Method), MethodB = unique(res.def$Method), budget = unique(res.def$budget))
beat.fun = function(x.budget, x.a, x.b) {
  mean(res.def[budget == x.budget, measure[Method==x.a] >= measure[Method==x.b], by = .(case, fold)]$V1)
}
beat.mat = Map(beat.fun, x.budget = combs$budget, x.a = combs$MethodA, x.b = combs$MethodB)
beat.mat = split(unlist(beat.mat), combs$budget)
beat.mat = lapply(beat.mat, function(x) {
  ms = unique(res.def$Method)
  matrix(x, nrow = length(ms), dimnames = list(ms,ms))
})
xtable(beat.mat[[1]], align = "l|rrrr") %>% print.xtable(booktabs = TRUE)
@
The table gives the fractions of instances where $acc_A \geq acc_B$. $A$ column, $B$ rows.
\end{frame}

\begin{frame}{Performance: Dominance Tested}
<<resultsDominanceTested, echo=FALSE, results='asis'>>=
beat.test.fun = function(x.budget, x.a, x.b) {
  tres = res.def[budget == x.budget, {
    x = measure[Method==x.a]
    y = measure[Method==x.b]
    if (all(x == y)) 1 else t.test(x = x, y = y, paired = TRUE, alternative = "greater")$p.value
  }, by = .(case)]$V1
  mean(!is.na(tres) & tres <= 0.05)
}
beat.test.mat = Map(beat.test.fun, x.budget = combs$budget, x.a = combs$MethodA, x.b = combs$MethodB)
beat.test.mat = split(unlist(beat.test.mat), combs$budget)
beat.test.mat = lapply(beat.test.mat, function(x) {
  ms = unique(res.def$Method)
  matrix(x, nrow = length(ms), dimnames = list(ms,ms))
})
xtable(beat.test.mat[[1]], align = "l|rrrr") %>% print.xtable(booktabs = TRUE)
@
The table gives the fractions of instances where $H_0: acc_A \leq acc_B$ is rejected by the paired $t$-test. $A$ column, $B$ rows.
\end{frame}

\begin{frame}{Which Learner Tuner Combination was the best?}
<<rankingCombinations, echo=FALSE>>=
tmp = copy(res.def)
tmp = tmp[budget == 50 & Method %in% c("caret grid", "mlrHyperopt", "caret random")]
tmp = tmp[learner %nin% c("extraTrees", "nnet", "xgboost")]
tmp = tmp[,ranking := myrank(measure, ties.method = "first"), by = .(fold, problem, budget)]
g = ggplot(tmp, aes(x = as.factor(ranking), fill = Method))
g + geom_bar()

g = ggplot(tmp, aes(x = as.factor(ranking), fill = Combination))
g = g + geom_bar()
plotly::ggplotly(g)
@
The table gives the fractions of instances where $H_0: acc_A \leq acc_B$ is rejected by the paired $t$-test. $A$ column, $B$ rows.
\end{frame}


\section{Conclusions}

\begin{frame}[allowframebreaks]{References}
  \printbibliography
\end{frame}

\end{document}
